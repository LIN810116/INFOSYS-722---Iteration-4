{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 4:  Marine Life Distribution\n",
    "\n",
    "Author: Chinchien Lin\n",
    "UPI: clin864\n",
    "Email: clin864@aucklanduni.ac.nz\n",
    "Student ID: 938149604\n",
    "GitHub: https://github.com/LIN810116/INFOSYS-722---Iteration-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()\n",
    "import pandas\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import abs\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "#Import dataset\n",
    "    #inferSchema: for CSV\n",
    "df = spark.read.csv('marineLife(I4).csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Row:\", df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data structure \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get general statistics \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get general statistics of longitude features\n",
    "df.select('WMostLong', 'EMostLong').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique value\n",
    "print(\"Number of unique value:\")\n",
    "print(\"Class:\", df.select('Class').distinct().count())\n",
    "print(\"Order:\", df.select('Order').distinct().count())\n",
    "print(\"Order:\", df.select('Family').distinct().count())\n",
    "print(\"Genus:\", df.select('Genus').distinct().count())\n",
    "print(\"Species:\", df.select('Species').distinct().count())\n",
    "print(\"Scientific names:\", df.select('Scientific names').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example: Class\n",
    "df.groupBy('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value\n",
    "pd = df.toPandas()\n",
    "pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \"NA\"\n",
    "NA_W = df.filter(\"WMostLong == 'NA'\").count()\n",
    "NA_E = df.filter(\"EMostLong == 'NA'\").count()\n",
    "print(\"Number of NA values in WMostLong: \", NA_W)\n",
    "print(\"Number of NA values in EMostLong: \", NA_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check \"NA\": East and Eest\n",
    "NA_WE = df.filter(\"WMostLong == 'NA' and EMostLong == 'NA'\").count()\n",
    "print(\"Number of NA values in WMostLong and EMostLong: \", NA_WE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outliers\n",
    "outliersW = df.filter(\"WMostLong > 180 or WMostLong < -180\").count()\n",
    "outliersE = df.filter(\"EMostLong > 180 or EMostLong < -180\").count()\n",
    "print(\"Outliers of WMostLong:\", outliersW)\n",
    "print(\"Outliers of EMostLong:\", outliersE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select useful parameters\n",
    "df_pre = df.select('Class', 'Order', 'Family', 'WMostLong', 'EMostLong')\n",
    "df_pre.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows which has nulls in WMostLong and EMostLong\n",
    "df_pre = df_pre.filter(\"WMostLong != '' and WMostLong != 'NA' and EMostLong != '' and EMostLong != 'NA'\")\n",
    "#filling missing values\n",
    "df_pre = df_pre.na.fill('Unknown', subset=['Class', 'Order', 'Family'])\n",
    "\n",
    "print(\"Number of Row:\", df_pre.count())\n",
    "# Check missing value\n",
    "pd_pre = df_pre.toPandas()\n",
    "print(\"Check null values:\")\n",
    "display(pd_pre.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#filling missing values\n",
    "df_pre = df_pre.na.fill('Unknown', subset=['Class', 'Order', 'Family'])\n",
    "# Check missing value\n",
    "pd_pre = df_pre.toPandas()\n",
    "pd_pre.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "df_pre = df_pre.filter(\"WMostLong < 180 and WMostLong > -180\")\n",
    "df_pre = df_pre.filter(\"EMostLong < 180 and EMostLong > -180\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data type for EMostLong and WMostLong\n",
    "df_pre = df_pre.withColumn(\"EMostLong\", df_pre.EMostLong.cast(DoubleType()))\n",
    "df_pre = df_pre.withColumn(\"WMostLong\", df_pre.WMostLong.cast(DoubleType()))\n",
    "# Create new columns\n",
    "# Coverage\n",
    "df_pre = df_pre.withColumn(\"Coverage\", abs(df_pre.EMostLong - df_pre.WMostLong))\n",
    "# Level\n",
    "df_pre = df_pre.withColumn(\"Level\", lit(df_pre.Coverage / 30))\n",
    "df_pre = df_pre.withColumn(\"Level\", df_pre.Level.cast(IntegerType()))\n",
    "#isWide\n",
    "df_pre = df_pre.withColumn(\"isWide\", lit(df_pre.Coverage / 180))\n",
    "df_pre = df_pre.withColumn(\"isWide\", df_pre.isWide.cast(IntegerType()))\n",
    "\n",
    "print(\"Number of Row:\", df_pre.count())\n",
    "df_pre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre = df_pre.select('Class', 'Order', 'Family', 'Level', 'isWide')\n",
    "df_pre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre.describe().show()\n",
    "df_pre.groupBy('isWide').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset 1: Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Level = df_pre.select('Class', 'Order', 'Family', 'Level')\n",
    "df_Level.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset 2: isWide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isWide = df_pre.select('Class', 'Order', 'Family', 'isWide')\n",
    "df_isWide.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_Level,testData_Level = df_Level.randomSplit([0.7,0.3])\n",
    "trainData_isWide,testData_isWide = df_isWide.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check out the count.\n",
    "trainData_Level.describe().show()\n",
    "testData_Level.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass:   Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "# First create a string indexer which converts every string into a number, such as male = 0 and female = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "Class_indexer = StringIndexer(inputCol='Class',outputCol='ClassIndex')\n",
    "Order_indexer = StringIndexer(inputCol='Order',outputCol='OrderIndex')\n",
    "Family_indexer = StringIndexer(inputCol='Family',outputCol='FamilyIndex')\n",
    "\n",
    "Level_indexer = StringIndexer(inputCol='Level',outputCol='label')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "# This makes it easier to process when you have multiple classes.\n",
    "Class_encoder = OneHotEncoder(inputCol='ClassIndex',outputCol='ClassVec')\n",
    "Order_encoder = OneHotEncoder(inputCol='OrderIndex',outputCol='OrderVec')\n",
    "Family_encoder = OneHotEncoder(inputCol='FamilyIndex',outputCol='FamilyVec')\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ClassVec','OrderVec','FamilyVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline_Level = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, Level_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline_Level.fit(df_Level)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_Level)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline_Level = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, Level_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline_Level.fit(df_Level)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_Level)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "(training, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))\n",
    "\n",
    "result = mlrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-class: Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "\n",
    "# First create a string indexer which converts every string into a number, such as male = 0 and female = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "Class_indexer = StringIndexer(inputCol='Class',outputCol='ClassIndex')\n",
    "Order_indexer = StringIndexer(inputCol='Order',outputCol='OrderIndex')\n",
    "Family_indexer = StringIndexer(inputCol='Family',outputCol='FamilyIndex')\n",
    "\n",
    "#Level_indexer = StringIndexer(inputCol='Level',outputCol='label')\n",
    "isWide_indexer = StringIndexer(inputCol='isWide',outputCol='label')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "# This makes it easier to process when you have multiple classes.\n",
    "Class_encoder = OneHotEncoder(inputCol='ClassIndex',outputCol='ClassVec')\n",
    "Order_encoder = OneHotEncoder(inputCol='OrderIndex',outputCol='OrderVec')\n",
    "Family_encoder = OneHotEncoder(inputCol='FamilyIndex',outputCol='FamilyVec')\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ClassVec','OrderVec','FamilyVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "#pipeline_Level = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, Level_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "pipeline = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, isWide_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "#pipeline_model = pipeline_Level.fit(df_Level)\n",
    "pipeline_model = pipeline.fit(df_isWide)\n",
    "\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_isWide)\n",
    "\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "#pipe_df_Level = pipe_df_Level.select('label', 'features')\n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data, test_data = pipe_df.randomSplit([0.7,0.3])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "# Instantiate the model.\n",
    "lr_model = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "# Fit the model.\n",
    "lr_model = lr_model.fit(train_data)\n",
    "\n",
    "# And evaluate the model using the test data.\n",
    "results = lr_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualising the coefficients. Sort from lowest to highest.\n",
    "beta = np.sort(lr_model.coefficients)\n",
    "\n",
    "# Plot the data.\n",
    "plt.plot(beta)\n",
    "\n",
    "# Add a label to the data.\n",
    "plt.ylabel('Beta Coefficients')\n",
    "\n",
    "# Show the graph. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a summary of the data.\n",
    "training_summary = lr_model.summary\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame.\n",
    "ROC = training_summary.roc.toPandas()\n",
    "\n",
    "# Plot the true positive and false positive rates.\n",
    "plt.plot(ROC['FPR'],ROC['TPR'])\n",
    "\n",
    "# Define the labels.\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC statistic. \n",
    "print('Area Under the Curve: ' + str(training_summary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Pandas DataFrame.\n",
    "pr = training_summary.pr.toPandas()\n",
    "\n",
    "# Plot model recall and precision.\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "\n",
    "# Define the labels and show the graph. \n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Class\" VS \"isWide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isWide2 = df_pre.select('Class', 'Order', 'Family', 'isWide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "# First create a string indexer which converts every string into a number, such as male = 0 and female = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "Class_indexer = StringIndexer(inputCol='Class',outputCol='ClassIndex')\n",
    "\n",
    "isWide_indexer = StringIndexer(inputCol='isWide',outputCol='label')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "# This makes it easier to process when you have multiple classes.\n",
    "Class_encoder = OneHotEncoder(inputCol='ClassIndex',outputCol='ClassVec')\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ClassVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline = Pipeline(stages=[Class_indexer, isWide_indexer, Class_encoder, assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline.fit(df_isWide2)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_isWide2)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
