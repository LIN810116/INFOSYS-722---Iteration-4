{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 4:  Marine Life Distribution\n",
    "\n",
    "Author: Chinchien Lin\n",
    "UPI: clin864\n",
    "Email: clin864@aucklanduni.ac.nz\n",
    "Student ID: 938149604\n",
    "GitHub: https://github.com/LIN810116/INFOSYS-722---Iteration-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()\n",
    "import pandas\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import abs\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "#Import dataset\n",
    "    #inferSchema: for CSV\n",
    "df = spark.read.csv('marineLife(I4).csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Row: 5634\n",
      "+----------+-----------+-----------------+---------------+-----------------+-------------+--------------------+---------+---------+\n",
      "| SpeciesID|      Class|            Order|         Family|            Genus|      Species|    Scientific names|WMostLong|EMostLong|\n",
      "+----------+-----------+-----------------+---------------+-----------------+-------------+--------------------+---------+---------+\n",
      "| Fis-29581|Actinopteri|      Perciformes|   Acanthuridae|       Acanthurus|    monroviae|Acanthurus monroviae|       NA|       NA|\n",
      "| Fis-26602|Actinopteri|Pleuronectiformes|      Achiridae|        Trinectes|  paulistanus|Trinectes paulist...|      -55|      -30|\n",
      "| Fis-27804|Actinopteri|Pleuronectiformes|      Achiridae|          Achirus|   mazatlanus|  Achirus mazatlanus|     -120|      -70|\n",
      "| Fis-28886|Actinopteri|Pleuronectiformes|      Achiridae|        Trinectes|   fimbriatus|Trinectes fimbriatus|       NA|       NA|\n",
      "| Fis-28885|Actinopteri|Pleuronectiformes|      Achiridae|          Achirus|       scutum|      Achirus scutum|       NA|       NA|\n",
      "| Fis-28883|Actinopteri|Pleuronectiformes|      Achiridae|          Achirus|  klunzingeri| Achirus klunzingeri|       NA|       NA|\n",
      "| Fis-28888|Actinopteri|Pleuronectiformes|      Achiridae|        Trinectes|  fonsecensis|Trinectes fonsece...|       NA|       NA|\n",
      "| Fis-25037|Actinopteri|Pleuronectiformes|      Achiridae|        Trinectes|    maculatus| Trinectes maculatus|      -79|      -74|\n",
      "|Fis-115342|Actinopteri|Pleuronectiformes|      Achiridae|      Gymnachirus|        melas|   Gymnachirus melas|      -98|      -73|\n",
      "| Fis-28887|Actinopteri|Pleuronectiformes|      Achiridae|        Trinectes|  fluviatilis|Trinectes fluviat...|       NA|       NA|\n",
      "|Fis-133769|Actinopteri|Pleuronectiformes|Achiropsettidae|      Mancopsetta|     maculata|Mancopsetta maculata|       NA|       NA|\n",
      "| Fis-32200|Actinopteri|Pleuronectiformes|Achiropsettidae|     Achiropsetta|  tricholepis|Achiropsetta tric...|     -180|      180|\n",
      "| Fis-32203|Actinopteri|Pleuronectiformes|Achiropsettidae|Pseudomancopsetta|  andriashevi|Pseudomancopsetta...|       NA|       NA|\n",
      "| Fis-58742|Actinopteri| Acipenseriformes|  Acipenseridae|        Acipenser|   oxyrinchus|Acipenser oxyrinchus|     -102|      -33|\n",
      "| Fis-32981|Actinopteri| Acipenseriformes|  Acipenseridae|        Acipenser|     sinensis|  Acipenser sinensis|      -97|      132|\n",
      "| Fis-29594|Actinopteri| Acipenseriformes|  Acipenseridae|        Acipenser|transmontanus|Acipenser transmo...|     -163|     -107|\n",
      "| Fis-29585|Actinopteri| Acipenseriformes|  Acipenseridae|        Acipenser| brevirostrum|Acipenser breviro...|      -82|      -65|\n",
      "| Fis-29588|Actinopteri| Acipenseriformes|  Acipenseridae|        Acipenser|  medirostris|Acipenser mediros...|     -166|     -114|\n",
      "| Fis-33633|Actinopteri| Acipenseriformes|  Acipenseridae|        Acipenser|      mikadoi|   Acipenser mikadoi|      125|      145|\n",
      "| Fis-29593|Actinopteri| Acipenseriformes|  Acipenseridae|        Acipenser|       sturio|    Acipenser sturio|      -25|       42|\n",
      "+----------+-----------+-----------------+---------------+-----------------+-------------+--------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Row:\", df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SpeciesID: string (nullable = true)\n",
      " |-- Class: string (nullable = true)\n",
      " |-- Order: string (nullable = true)\n",
      " |-- Family: string (nullable = true)\n",
      " |-- Genus: string (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      " |-- Scientific names: string (nullable = true)\n",
      " |-- WMostLong: string (nullable = true)\n",
      " |-- EMostLong: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data structure \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------+----------------+------------+-------------+-------+--------------------+-----------------+------------------+\n",
      "|summary|SpeciesID|       Class|           Order|      Family|        Genus|Species|    Scientific names|        WMostLong|         EMostLong|\n",
      "+-------+---------+------------+----------------+------------+-------------+-------+--------------------+-----------------+------------------+\n",
      "|  count|     5631|        5631|            5631|        5631|         5631|   5631|                5631|             5631|              5631|\n",
      "|   mean|     null|        null|            null|        null|         null|   null|                null|-17.3216218097304|-2.333721291123526|\n",
      "| stddev|     null|        null|            null|        null|         null|   null|                null| 93.1836869615287|114.34758239286916|\n",
      "|    min|Fis-10312| Actinopteri|Acipenseriformes|Acanthuridae|    Abalistes|abaster| Abalistes stellaris|               -1|                -1|\n",
      "|    max| Fis-9837|Petromyzonti|       Zeiformes|   Zoarcidae|Zosterisessor| zysron|Zosterisessor oph...|               NA|                NA|\n",
      "+-------+---------+------------+----------------+------------+-------------+-------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get general statistics \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|        WMostLong|         EMostLong|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|             5631|              5631|\n",
      "|   mean|-17.3216218097304|-2.333721291123526|\n",
      "| stddev| 93.1836869615287|114.34758239286916|\n",
      "|    min|               -1|                -1|\n",
      "|    max|               NA|                NA|\n",
      "+-------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get general statistics of longitude features\n",
    "df.select('WMostLong', 'EMostLong').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique value:\n",
      "Class: 7\n",
      "Order: 58\n",
      "Order: 355\n",
      "Genus: 1812\n",
      "Species: 3961\n",
      "Scientific names: 5632\n"
     ]
    }
   ],
   "source": [
    "# check unique value\n",
    "print(\"Number of unique value:\")\n",
    "print(\"Class:\", df.select('Class').distinct().count())\n",
    "print(\"Order:\", df.select('Order').distinct().count())\n",
    "print(\"Order:\", df.select('Family').distinct().count())\n",
    "print(\"Genus:\", df.select('Genus').distinct().count())\n",
    "print(\"Species:\", df.select('Species').distinct().count())\n",
    "print(\"Scientific names:\", df.select('Scientific names').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         Class|count|\n",
      "+--------------+-----+\n",
      "|Elasmobranchii|  472|\n",
      "|          null|    3|\n",
      "|   Actinopteri| 5127|\n",
      "|        Myxini|    7|\n",
      "|   Holocephali|   18|\n",
      "|   Coelacanthi|    1|\n",
      "|  Petromyzonti|    6|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for example: Class\n",
    "df.groupBy('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeciesID           3\n",
       "Class               3\n",
       "Order               3\n",
       "Family              3\n",
       "Genus               3\n",
       "Species             3\n",
       "Scientific names    3\n",
       "WMostLong           3\n",
       "EMostLong           3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing value\n",
    "pd = df.toPandas()\n",
    "pd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in WMostLong:  2404\n",
      "Number of NA values in EMostLong:  2409\n"
     ]
    }
   ],
   "source": [
    "# Check \"NA\"\n",
    "NA_W = df.filter(\"WMostLong == 'NA'\").count()\n",
    "NA_E = df.filter(\"EMostLong == 'NA'\").count()\n",
    "print(\"Number of NA values in WMostLong: \", NA_W)\n",
    "print(\"Number of NA values in EMostLong: \", NA_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NA values in WMostLong and EMostLong:  2402\n"
     ]
    }
   ],
   "source": [
    "# Check \"NA\": East and Eest\n",
    "NA_WE = df.filter(\"WMostLong == 'NA' and EMostLong == 'NA'\").count()\n",
    "print(\"Number of NA values in WMostLong and EMostLong: \", NA_WE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers of WMostLong: 0\n",
      "Outliers of EMostLong: 0\n"
     ]
    }
   ],
   "source": [
    "# Check outliers\n",
    "outliersW = df.filter(\"WMostLong > 180 or WMostLong < -180\").count()\n",
    "outliersE = df.filter(\"EMostLong > 180 or EMostLong < -180\").count()\n",
    "print(\"Outliers of WMostLong:\", outliersW)\n",
    "print(\"Outliers of EMostLong:\", outliersE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------------+---------+---------+\n",
      "|      Class|            Order|      Family|WMostLong|EMostLong|\n",
      "+-----------+-----------------+------------+---------+---------+\n",
      "|Actinopteri|      Perciformes|Acanthuridae|       NA|       NA|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|      -55|      -30|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|     -120|      -70|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|       NA|       NA|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|       NA|       NA|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|       NA|       NA|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|       NA|       NA|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|      -79|      -74|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|      -98|      -73|\n",
      "|Actinopteri|Pleuronectiformes|   Achiridae|       NA|       NA|\n",
      "+-----------+-----------------+------------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select useful parameters\n",
    "df_pre = df.select('Class', 'Order', 'Family', 'WMostLong', 'EMostLong')\n",
    "df_pre.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Row: 3220\n",
      "Check null values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Class        0\n",
       "Order        0\n",
       "Family       0\n",
       "WMostLong    0\n",
       "EMostLong    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove the rows which has nulls in WMostLong and EMostLong\n",
    "df_pre = df_pre.filter(\"WMostLong != '' and WMostLong != 'NA' and EMostLong != '' and EMostLong != 'NA'\")\n",
    "#filling missing values\n",
    "df_pre = df_pre.na.fill('Unknown', subset=['Class', 'Order', 'Family'])\n",
    "\n",
    "print(\"Number of Row:\", df_pre.count())\n",
    "# Check missing value\n",
    "pd_pre = df_pre.toPandas()\n",
    "print(\"Check null values:\")\n",
    "display(pd_pre.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class        0\n",
       "Order        0\n",
       "Family       0\n",
       "WMostLong    0\n",
       "EMostLong    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filling missing values\n",
    "df_pre = df_pre.na.fill('Unknown', subset=['Class', 'Order', 'Family'])\n",
    "# Check missing value\n",
    "pd_pre = df_pre.toPandas()\n",
    "pd_pre.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "df_pre = df_pre.filter(\"WMostLong < 180 and WMostLong > -180\")\n",
    "df_pre = df_pre.filter(\"EMostLong < 180 and EMostLong > -180\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Row: 3021\n",
      "+-----------+-----------------+-------------+---------+---------+--------+-----+------+\n",
      "|      Class|            Order|       Family|WMostLong|EMostLong|Coverage|Level|isWide|\n",
      "+-----------+-----------------+-------------+---------+---------+--------+-----+------+\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    -55.0|    -30.0|    25.0|    0|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|   -120.0|    -70.0|    50.0|    1|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    -79.0|    -74.0|     5.0|    0|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    -98.0|    -73.0|    25.0|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|   -102.0|    -33.0|    69.0|    2|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    -97.0|    132.0|   229.0|    7|     1|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|   -163.0|   -107.0|    56.0|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    -82.0|    -65.0|    17.0|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|   -166.0|   -114.0|    52.0|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    125.0|    145.0|    20.0|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    -25.0|     42.0|    67.0|    2|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     17.0|     60.0|    43.0|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     26.0|     54.0|    28.0|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     14.0|     53.0|    39.0|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     22.0|     54.0|    32.0|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     10.0|     20.0|    10.0|    0|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|     99.0|    155.0|    56.0|    1|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|    -19.0|     15.0|    34.0|    1|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|     19.0|   -154.0|   173.0|    5|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|    -93.0|     14.0|   107.0|    3|     0|\n",
      "+-----------+-----------------+-------------+---------+---------+--------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting the data type for EMostLong and WMostLong\n",
    "df_pre = df_pre.withColumn(\"EMostLong\", df_pre.EMostLong.cast(DoubleType()))\n",
    "df_pre = df_pre.withColumn(\"WMostLong\", df_pre.WMostLong.cast(DoubleType()))\n",
    "# Create new columns\n",
    "# Coverage\n",
    "df_pre = df_pre.withColumn(\"Coverage\", abs(df_pre.EMostLong - df_pre.WMostLong))\n",
    "# Level\n",
    "df_pre = df_pre.withColumn(\"Level\", lit(df_pre.Coverage / 30))\n",
    "df_pre = df_pre.withColumn(\"Level\", df_pre.Level.cast(IntegerType()))\n",
    "#isWide\n",
    "df_pre = df_pre.withColumn(\"isWide\", lit(df_pre.Coverage / 180))\n",
    "df_pre = df_pre.withColumn(\"isWide\", df_pre.isWide.cast(IntegerType()))\n",
    "\n",
    "print(\"Number of Row:\", df_pre.count())\n",
    "df_pre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Class: string (nullable = false)\n",
      " |-- Order: string (nullable = false)\n",
      " |-- Family: string (nullable = false)\n",
      " |-- WMostLong: double (nullable = true)\n",
      " |-- EMostLong: double (nullable = true)\n",
      " |-- Coverage: double (nullable = true)\n",
      " |-- Level: integer (nullable = true)\n",
      " |-- isWide: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pre.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-------------+-----+------+\n",
      "|      Class|            Order|       Family|Level|isWide|\n",
      "+-----------+-----------------+-------------+-----+------+\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    0|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    1|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    0|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    2|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    7|     1|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    2|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    0|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    1|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    0|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|    1|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|    1|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|    5|     0|\n",
      "|Actinopteri|      Perciformes|Acropomatidae|    3|     0|\n",
      "+-----------+-----------------+-------------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pre = df_pre.select('Class', 'Order', 'Family', 'Level', 'isWide')\n",
    "df_pre.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------------+------------+------------------+-------------------+\n",
      "|summary|       Class|           Order|      Family|             Level|             isWide|\n",
      "+-------+------------+----------------+------------+------------------+-------------------+\n",
      "|  count|        3021|            3021|        3021|              3021|               3021|\n",
      "|   mean|        null|            null|        null| 2.562727573651109|0.16120489904005297|\n",
      "| stddev|        null|            null|        null|2.6081484085060116|0.36778071414614144|\n",
      "|    min| Actinopteri|Acipenseriformes|Acanthuridae|                 0|                  0|\n",
      "|    max|Petromyzonti|       Zeiformes|   Zoarcidae|                11|                  1|\n",
      "+-------+------------+----------------+------------+------------------+-------------------+\n",
      "\n",
      "+------+-----+\n",
      "|isWide|count|\n",
      "+------+-----+\n",
      "|     1|  487|\n",
      "|     0| 2534|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pre.describe().show()\n",
    "df_pre.groupBy('isWide').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset 1: Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-------------+-----+\n",
      "|      Class|            Order|       Family|Level|\n",
      "+-----------+-----------------+-------------+-----+\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    1|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|    0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    2|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    7|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    1|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    1|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|    0|\n",
      "+-----------+-----------------+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Level = df_pre.select('Class', 'Order', 'Family', 'Level')\n",
    "df_Level.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset 2: isWide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-------------+------+\n",
      "|      Class|            Order|       Family|isWide|\n",
      "+-----------+-----------------+-------------+------+\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|     0|\n",
      "|Actinopteri|Pleuronectiformes|    Achiridae|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     1|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     0|\n",
      "|Actinopteri| Acipenseriformes|Acipenseridae|     0|\n",
      "+-----------+-----------------+-------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_isWide = df_pre.select('Class', 'Order', 'Family', 'isWide')\n",
    "df_isWide.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_Level,testData_Level = df_Level.randomSplit([0.7,0.3])\n",
    "trainData_isWide,testData_isWide = df_isWide.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------------+------------+------------------+\n",
      "|summary|       Class|           Order|      Family|             Level|\n",
      "+-------+------------+----------------+------------+------------------+\n",
      "|  count|        2088|            2088|        2088|              2088|\n",
      "|   mean|        null|            null|        null|2.5613026819923372|\n",
      "| stddev|        null|            null|        null| 2.597790633355744|\n",
      "|    min| Actinopteri|Acipenseriformes|Acanthuridae|                 0|\n",
      "|    max|Petromyzonti|       Zeiformes|   Zoarcidae|                11|\n",
      "+-------+------------+----------------+------------+------------------+\n",
      "\n",
      "+-------+------------+----------------+------------+------------------+\n",
      "|summary|       Class|           Order|      Family|             Level|\n",
      "+-------+------------+----------------+------------+------------------+\n",
      "|  count|         933|             933|         933|               933|\n",
      "|   mean|        null|            null|        null|2.5659163987138265|\n",
      "| stddev|        null|            null|        null| 2.632578196931748|\n",
      "|    min| Actinopteri|Acipenseriformes|Acanthuridae|                 0|\n",
      "|    max|Petromyzonti|       Zeiformes|      Zeidae|                11|\n",
      "+-------+------------+----------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check out the count.\n",
    "trainData_Level.describe().show()\n",
    "testData_Level.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass:   Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "# First create a string indexer which converts every string into a number, such as male = 0 and female = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "Class_indexer = StringIndexer(inputCol='Class',outputCol='ClassIndex')\n",
    "Order_indexer = StringIndexer(inputCol='Order',outputCol='OrderIndex')\n",
    "Family_indexer = StringIndexer(inputCol='Family',outputCol='FamilyIndex')\n",
    "\n",
    "Level_indexer = StringIndexer(inputCol='Level',outputCol='label')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "# This makes it easier to process when you have multiple classes.\n",
    "Class_encoder = OneHotEncoder(inputCol='ClassIndex',outputCol='ClassVec')\n",
    "Order_encoder = OneHotEncoder(inputCol='OrderIndex',outputCol='OrderVec')\n",
    "Family_encoder = OneHotEncoder(inputCol='FamilyIndex',outputCol='FamilyVec')\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ClassVec','OrderVec','FamilyVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline_Level = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, Level_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline_Level.fit(df_Level)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_Level)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline_Level = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, Level_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline_Level.fit(df_Level)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_Level)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(340,[0,5,60],[1....|\n",
      "|           0.0|  0.0|(340,[0,5,60],[1....|\n",
      "|           0.0|  0.0|(340,[0,5,60],[1....|\n",
      "|           0.0|  0.0|(340,[0,5,60],[1....|\n",
      "|           0.0|  0.0|(340,[0,5,60],[1....|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.69747\n",
      "RandomForestClassificationModel (uid=rfc_d286495ea276) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         0.0|(340,[0,5,60],[1....|\n",
      "|       0.0|         0.0|(340,[0,5,60],[1....|\n",
      "|       0.0|         0.0|(340,[0,5,60],[1....|\n",
      "|       0.0|         0.0|(340,[0,5,60],[1....|\n",
      "|       0.0|         0.0|(340,[0,5,60],[1....|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.702299 \n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4b768520f0bec584ef01) of depth 5 with 25 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial coefficients: DenseMatrix([[0., 0., 0., ..., 0., 0., 0.],\n",
      "             [0., 0., 0., ..., 0., 0., 0.],\n",
      "             [0., 0., 0., ..., 0., 0., 0.],\n",
      "             ...,\n",
      "             [0., 0., 0., ..., 0., 0., 0.],\n",
      "             [0., 0., 0., ..., 0., 0., 0.],\n",
      "             [0., 0., 0., ..., 0., 0., 0.]])\n",
      "Multinomial intercepts: [1.8254897085129091,1.450999133124913,1.1883008740998784,0.47490561967224343,0.20744445779139858,0.13210724881689784,0.0786271534584337,-0.04826880363528784,-0.4619020482081257,-0.8780315760531435,-1.8803094919601404,-2.089362275619977]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "(training, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))\n",
    "\n",
    "result = mlrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-class: Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "\n",
    "# First create a string indexer which converts every string into a number, such as male = 0 and female = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "Class_indexer = StringIndexer(inputCol='Class',outputCol='ClassIndex')\n",
    "Order_indexer = StringIndexer(inputCol='Order',outputCol='OrderIndex')\n",
    "Family_indexer = StringIndexer(inputCol='Family',outputCol='FamilyIndex')\n",
    "\n",
    "#Level_indexer = StringIndexer(inputCol='Level',outputCol='label')\n",
    "isWide_indexer = StringIndexer(inputCol='isWide',outputCol='label')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "# This makes it easier to process when you have multiple classes.\n",
    "Class_encoder = OneHotEncoder(inputCol='ClassIndex',outputCol='ClassVec')\n",
    "Order_encoder = OneHotEncoder(inputCol='OrderIndex',outputCol='OrderVec')\n",
    "Family_encoder = OneHotEncoder(inputCol='FamilyIndex',outputCol='FamilyVec')\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ClassVec','OrderVec','FamilyVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "#pipeline_Level = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, Level_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "pipeline = Pipeline(stages=[Class_indexer, Order_indexer, Family_indexer, isWide_indexer, Class_encoder, Order_encoder, Family_encoder, assembler])\n",
    "\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "#pipeline_model = pipeline_Level.fit(df_Level)\n",
    "pipeline_model = pipeline.fit(df_isWide)\n",
    "\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_isWide)\n",
    "\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "#pipe_df_Level = pipe_df_Level.select('label', 'features')\n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 2090\n",
      "Test Dataset Count: 931\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data, test_data = pipe_df.randomSplit([0.7,0.3])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "# Instantiate the model.\n",
    "lr_model = LogisticRegression(featuresCol='features',labelCol='label')\n",
    "\n",
    "# Fit the model.\n",
    "lr_model = lr_model.fit(train_data)\n",
    "\n",
    "# And evaluate the model using the test data.\n",
    "results = lr_model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualising the coefficients. Sort from lowest to highest.\n",
    "beta = np.sort(lr_model.coefficients)\n",
    "\n",
    "# Plot the data.\n",
    "plt.plot(beta)\n",
    "\n",
    "# Add a label to the data.\n",
    "plt.ylabel('Beta Coefficients')\n",
    "\n",
    "# Show the graph. \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under the Curve: 0.8116046831955922\n"
     ]
    }
   ],
   "source": [
    "# Let's get a summary of the data.\n",
    "training_summary = lr_model.summary\n",
    "\n",
    "# Convert the DataFrame to a Pandas DataFrame.\n",
    "ROC = training_summary.roc.toPandas()\n",
    "\n",
    "# Plot the true positive and false positive rates.\n",
    "plt.plot(ROC['FPR'],ROC['TPR'])\n",
    "\n",
    "# Define the labels.\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Print the AUC statistic. \n",
    "print('Area Under the Curve: ' + str(training_summary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VPW9//HXJxshLAlkI5AFCKAgi0BkraJ1A63a6tXrVmtrq7W1rbe9dvnVe7W2t729t+3tfqtVq/VK61pFhbpbFdlB9sWwhy0kQNjJ9vn9MUNKUcgkzOTMJO/n4zEP5kzOzHwOoG++6zF3R0REBCAp6AJERCR+KBRERKSJQkFERJooFEREpIlCQUREmigURESkiUJBRESaKBRERKSJQkFERJqkBF1AS+Xk5Hjfvn2DLkNEJKEsWLCgyt1zmzsv4UKhb9++zJ8/P+gyREQSipltjOQ8dR+JiEgThYKIiDRRKIiISBOFgoiINFEoiIhIk5iFgpk9bGaVZrbsBD83M/ulmZWb2RIzGxWrWkREJDKxbCk8Akw+yc+nAAPDj1uB/41hLSIiEoGYrVNw97fNrO9JTrkC+KOH7gc628yyzKzA3bfFop55G3bxzpqdp/QZk07LY3RJjyhVJCISf4JcvNYH2HzMcUX4tQ+FgpndSqg1QXFxcau+bOHG3fzqzfJWvRfAHd4tr+LZL01s9WeIiMS7hFjR7O4PAA8AlJWVeWs+47ZJpdw2qbTVNXz3L0t5cck23B0za/XniIjEsyBnH20Bio45Lgy/FpdKc7tSc6iO6gO1QZciIhIzQYbCNOCm8CykcUBNrMYToqE0rysAayv3B1yJiEjsxKz7yMz+BJwL5JhZBXAPkArg7r8DpgOXAOXAQeCzsaolGkpzuwCwducBxvbPDrgaEZHYiOXso+ua+bkDX47V90db78zOdE5NplwtBRFpx7SiOUJJSUb/3C6s3alQEJH2S6HQAqW5XRUKItKuKRRaoDS3K1v2HOJQbUPQpYiIxIRCoQVK87rgDuurDgRdiohITCgUWqA0NzQttVxdSCLSTikUWqBfThfM0AwkEWm3FAotkJ6aTL+cLqzYujfoUkREYkKh0EJnFmXx/uY9hJZZiIi0LwqFFjqzKIuq/UfYWnM46FJERKJOodBCZxZlAfD+pj0BVyIiEn0KhRY6vVd30lKSWFyhUBCR9keh0EJpKUmc0bs7729WKIhI+6NQaIURhVksraihvqEx6FJERKJKodAKI4uzOFTXwAdaryAi7YxCoRVGFIYGm+dv3B1wJSIi0aVQaIWS7AyKenbmjZU7gi5FRCSqFAqtYGZcOLgXM9dWc+BIfdDliIhEjUKhlS4ckk9tfSPvfLAz6FJERKJGodBKZ/XtQVZGKq+sUBeSiLQfCoVWSklO4uOn5fHGqkpNTRWRdkOhcAouHJLPnoN1moUkIu2GQuEUnDMol7TkJF5VF5KItBMKhVPQpVMKEwZk8+qKHdpKW0TaBYXCKbpwSD6bdh1kzQ6tbhaRxKdQOEUXDM4H4NUV2wOuRETk1CkUTlF+93RGFGVpXEFE2gWFQhRcNCSfxRU1bN51MOhSREROiUIhCq4c1YfkJOP/Zm8MuhQRkVOiUIiCgszOTD6jF3+au4mDtdoLSUQSl0IhSm6e2Je9h+t5btHWoEsREWk1hUKUlJX04Ize3fnDzPU0NGrNgogkJoVClJgZt59bygeV+3lmYUXQ5YiItIpCIYouHVbAyOIsfvLyat1nQUQSkkIhisyMuy8dQuW+I9z/9rqgyxERaTGFQpSNLunBJ4YX8MDba9lWcyjockREWkShEAPfmnw6jQ7//fLqoEsREWkRhUIMFPXM4HMT+/Hswi0sqdgTdDkiIhFTKMTIl84rJbtLGv/vL0uprded2UQkMcQ0FMxsspmtNrNyM/v2R/y82MzeNLNFZrbEzC6JZT1tqXt6Kj+8chjLtuzlp6+oG0lEEkPMQsHMkoHfAFOAIcB1ZjbkuNPuBp5095HAtcBvY1VPEC4+oxc3jC3m/rfX8V55VdDliIg0K5YthTFAubuvc/da4M/AFced40D38PNMoN3tEXH3pUPom53B3c8vUzeSiMS9WIZCH2DzMccV4deOdS9wo5lVANOBr3zUB5nZrWY238zm79y5Mxa1xkzntGTuuewM1u08wKPvbQi6HBGRkwp6oPk64BF3LwQuAR4zsw/V5O4PuHuZu5fl5ua2eZGn6rzT8/j46Xn84vUPqNx3OOhyREROKJahsAUoOua4MPzasW4BngRw91lAOpATw5oC82+fGEJtfSP/9VcNOotI/IplKMwDBppZPzNLIzSQPO24czYB5wOY2WBCoZBY/UMR6pfThVvO7sfTCypYuGl30OWIiHykmIWCu9cDdwAvAysJzTJabmb3mdnl4dO+AXzBzBYDfwJudvd2u+/0HecNIL97J+6dtpxGba8tInEoJZYf7u7TCQ0gH/vavx/zfAUwMZY1xJMunVL4zpTB3PnE+zy9oIJrzipq/k0iIm0o6IHmDueKM3szuqQHP/7rKmoO1QVdjojIP1AotDEz43uXn8Gug7X84rUPgi5HROQfKBQCMLRPJteeVcQfZ21ge42mqIpI/FAoBOT2SQNocGfqnI1BlyIi0kShEJDi7Aw+floeU+du4kh9Q9DliIgACoVA3TShL1X7a5mxdHvQpYiIAAqFQJ09IIf+OV14dNaGoEsREQEUCoFKSjI+Pb6ERZv26A5tIhIXFAoBu2p0IRlpyTwyc0PQpYiIKBSC1j09lWvKinh+8VY2Vh8IuhwR6eAUCnHg9nNLSUkyfvVGedCliEgHp1CIA/nd07lxXAl/WbSF9VVqLYhIcBQKceKLk0pJTTZ+8rLutyAiwVEoxIncbp348rkDeGnpNt5cVRl0OSLSQSkU4shtk0oZmNeVu59bxsHa+qDLEZEOSKEQR9JSkvjhlcPYsucQ//PqmqDLEZEOSKEQZ87q25Prxxbz0LvrWbalJuhyRKSDUSjEoW9NPp3srp34zrNLqW9oDLocEelAFApxKLNzKvdcNoSlW2p4dJa21haRtqNQiFOXDivgvNNy+ekrq7XSWUTajEIhTpkZP/jUMFKTk7jtsQWajSQibUKhEMf6ZHXml9eNZPWOfXzrmaW4e9AliUg7p1CIc5MG5XLXxafxwuKt/P6ddUGXIyLtXErQBUjzbp9UyrItNfxoxir6ZGVw6fCCoEsSkXZKLYUEYGb89OozGV3cgzufWMTba3YGXZKItFMKhQTROS2Zh24+iwF53bjtsQUs2Lgr6JJEpB2KOBTMrI+ZTTCzc44+YlmYfFhm51T++Lkx9MpM5zMPz2PuegWDiERXRKFgZj8GZgJ3A3eFH/8aw7rkBHK7dWLqF8aS170TNz08hzdXa0dVEYmeSFsKnwROc/dL3P2y8OPyWBYmJ1aQ2ZknbxtP/5yufP7R+fzfbK16FpHoiDQU1gGpsSxEWianayee/OJ4zh6Yw93PLeM/XlpBQ6PWMYjIqYl0SupB4H0zex04cvRFd/9qTKqSiHTtlMKDN5Xx/RdX8Pt31rOx+iA/v/ZMMtI001hEWifS/3tMCz8kzqQkJ/G9K4bSN6cL9724gn++fzYPfaaMvO7pQZcmIgnIIt06wczSgEHhw9XuXhezqk6irKzM58+fH8RXx73XVuzgq39eRFbnVB66+SwGF3QPuiQRiRNmtsDdy5o7L9LZR+cCHwC/AX4LrNGU1PhzwZB8nrxtPA3uXP27WbylmUki0kKRDjT/FLjI3Se5+znAxcD/xK4saa2hfTJ57ssTKe6ZwecemcdjmpkkIi0QaSikuvvqowfuvgbNRopbBZmdeeqL4zn3tDz+7bllfP9FzUwSkchEGgrzzexBMzs3/Pg9oI79ONalUwq/v6mMmyf05aF313PTw3P4YMe+oMsSkTgXaSjcDqwAvhp+rAi/dlJmNtnMVptZuZl9+wTnXGNmK8xsuZlNjbRwaV5yknHv5Wfww08NY0lFDZN/8Q73PL+M3Qdqgy5NROJUxLOPWvzBZsnAGuBCoAKYB1zn7iuOOWcg8CTwcXffbWZ57n7S0VHNPmqdXQdq+Z9X1/D4nI10S0/lzgsGcuO4ElKTtSeiSEcQldlHZvZk+NelZrbk+Ecznz0GKHf3de5eC/wZuOK4c74A/MbddwM0FwjSej27pPH9Tw5lxtfOYVifTL73wgou/vnbvLlKv+Ui8nfNLV77WvjXT7Tis/sAm485rgDGHnfOIAAzmwkkA/e6+19b8V0SodN6deOxW8bwxqpKfvDSSj77yDwmDcrl7ksHMzC/W9DliUjATtpScPdt4adVwGZ33wh0AkYAW6Pw/SnAQOBc4Drg92aWdfxJZnarmc03s/k7d+oGM6fKzDh/cD4v33kOd186mIWbdjP5F+9w77TlGm8Q6eAi7VB+G0g3sz7AK8CngUeaec8WoOiY48Lwa8eqAKa5e527ryc0BjHw+A9y9wfcvczdy3JzcyMsWZqTlpLE58/uz9/uOo/rxxTzx1kbOPcnb/GHmeupa2gMujwRCUCkoWDufhC4Evitu18NnNHMe+YBA82sX3iLjGv58P5JzxFqJWBmOYS6k3R3+jb2UeMNk3/+tu7VINIBRRwKZjYeuAF4Kfxa8sne4O71wB3Ay8BK4El3X25m95nZ0XsxvAxUm9kK4E3gLnevbulFSHQcHW946DNlNDp89g/z+Pyj89leczjo0kSkjUQ0JdXMJgHfAGa6+4/NrD9wZxBbZ2tKatuorW/kkffW87NX15CanMTdlw7mmrIizCzo0kSkFSKdkhqzdQqxolBoWxuqDvCtZ5YwZ/0uPjYghx9dOYyinhlBlyUiLRSVUDCzn7v7nWb2AvChE4O4JadCoe01NjpT527iR9NX4sC3Jp/Op8eVkJSkVoNIoog0FJpbp/BY+NefnHpJkqiSkowbx5Vw3ul5fOfZpdwzbTkvLtnKj68aTv/crkGXJyJRFOmYQhfgkLs3ho+TgU7hGUltSi2FYLk7zyzcwn0vLOdIfSP/cuEgPv+xfqRouwyRuBbVm+wArwPHdiR3Bl5rTWGS2MyMfxpdyGtfn8SkQbn854xVXPm/77F6u3ZgFWkPIg2FdHfff/Qg/FyjjR1YXvd07v/0aH59/Ui27D7EFb95l78sqgi6LBE5RZGGwgEzG3X0wMxGA4diU5IkCjPjE8N7M+POsxlemMW/PLGYe55fRm29VkOLJKrmBpqPuhN4ysy2Agb0Av45ZlVJQsnrls7jnx/Lj2es4sF317Ns615+e8Mo8runB12aiLRQxOsUzCwVOC18uNrd62JW1UlooDm+vbhkK998egkZaSn8+vqRjOufHXRJIkKUB5rNLAP4FvA1d18G9DWz1mynLe3cJ4b35vkvT6R7ego3PDiHB99ZR6ItkBTpyCIdU/gDUAuMDx9vAX4Qk4ok4Q3M78bzd0zkgsF5/OClldzxp0UcOFIfdFkiEoFIQ6HU3f8LqAMIr0/QclY5oW7pqfzuxtF8a/LpzFi6jU/+ZiZrd+5v/o0iEqhIQ6HWzDoT3urCzEqBIzGrStoFM+P2c0t57JaxVB+o5Ypfz2TG0m3Nv1FEAhNpKNwD/BUoMrPHCS1m+2bMqpJ2ZeKAHF78yscozevK7Y8v5BtPLqbmUCDzFESkGc3OPrLQXsmFwEFgHKFuo9nuXhX78j5Ms48SV219I7964wN++9Za8rp14sdXDeecQbqTnkhbiNrsIw+lxnR3r3b3l9z9xaACQRJbWkoS37joNJ69fQJdOqVw08Nz+ebTi6ner55IkXgRaffRQjM7K6aVSIcxoiiLF7/yMb44qZRnF27h4z/9G4/P2UhDo6auigQt0l1SVwEDgQ3AAUJdSO7uw2Na3UdQ91H78sGOffzb88uYvW4XIwoz+e6lQxjTr2fQZYm0O1G985qZlXzU6+6+sRW1nRKFQvvj7kxbvJUfTl/Jjr1HOO+0XO66+HSG9O4edGki7Ua07ryWDnwRGAAsBR5y90BXISkU2q9DtQ08OmsDv32znH1H6rl8RG++fuEgSrK7BF2aSMKLVig8QWjB2jvAFGCju38talW2gkKh/as5WMf9b6/l4ZnrqW9wri4r4tZz+tMvR+Eg0lrRCoWl7j4s/DwFmOvuo074hjagUOg4Kvce5pdvfMCT8yuoa2jkoiH53HpOKaNLegRdmkjCidY9mptWGLl7fWjJgkjbyOuezg8+OYyvnT+IR9/bwGOzN/Ly8h2UlfTg1nP6c8HgfJKS9HdSJJqaayk0EJptBKEZR50JLWI7OvuozUcC1VLouA4cqeeJeZt56N31bNlziOKeGXzp3FKuGl1Iqu4RLXJSUZ19FE8UClLf0MiMZdt58J11LK6ooahnZ7768YF8amQfUhQOIh9JoSDtnrvz5upKfvbqGpZt2UtRz85cN6aYq0cXkdutU9DlicQVhYJ0GO7OaysrefCddcxZv4vUZOOiM3pxw9hixvfPRmNhItEbaBaJe2bGhUPyuXBIPuWV+5g6ZzPPLKzgpSXb6J/ThevHFnPVqEJ6dEkLulSRuKeWgrRLh+saeGnJNqbO3cSCjbtJS0ni0mEF3DC2mNElPdR6kA5H3UciYSu37WXqnE38ZdEW9h+pZ1B+V64fU8ynRhWS2Tk16PJE2oRCQeQ4B47U88LirUydu4klFTWkpyZx2fDe3DCuhBGFmWo9SLumUBA5iaUVNUydu5Hn39/KwdoGhhR054ZxxVxxZh+6dtJQm7Q/CgWRCOw7XMdz72/l8dkbWbV9H13SkrliZB+uH1PM0D6ZQZcnEjUKBZEWcHcWbd7D47M38eKSrRypb2REURY3jCnmEyMKyEhT60ESm0JBpJVqDtbxzMIKps7dRHnlfrqlp3DlyD5cP7aE03p1C7o8kVZRKIicIndn7vpdTJ27iRlLt1Pb0EhZSQ9uGFfMlKEFpKcmB12iSMQUCiJRtOtALU8v2MzUOZvYUH2QrIxU/mlUIdeNLaY0t2vQ5Yk0S6EgEgONjc6sddU8PmcjryzfQX2jM75/NtecVcjFZ/TS2IPErbgIBTObDPwCSAYedPf/PMF5VwFPA2e5+0n/j69QkHhRue8wT82v4E9zN1Gx+xBd0pK5ZFgBV40uZEzfnrrXg8SVwEPBzJKBNcCFQAUwD7jO3Vccd1434CUgDbhDoSCJprHRmbthF88sqGD60m0cqG2gsEdnrhzZh6tGF+oe0xIX4iEUxgP3uvvF4ePvALj7j4477+fAq8BdwL8qFCSRHapt4OXl23lmYQXvllfhDmP79eSasiKmDFP3kgQnHnZJ7QNsPua4Ahh77AlmNgoocveXzOyuGNYi0iY6pyXzyZF9+OTIPmyrOcSzC7fw1PzNfOOpxdwzbTmXjSjg6rIiRhZlaVsNiUuB/bPFzJKAnwE3R3DurcCtAMXFxbEtTCRKCjI78+XzBvClc0uZt2E3T87fzHOLtvKnuZsZkNeVq0cXcunwAgp7ZARdqkiTwLqPzCwTWAvsD7+lF7ALuPxkXUjqPpJEtv9IPS8t2cqT8ytYsHE3AMMLM5k8tBdThhbQL0fjDxIb8TCmkEJooPl8YAuhgebr3X35Cc5/C40pSAeysfoAM5ZtZ8ay7SzevAeA03t1Y8rQAi4Z1ouB+Vo9LdETeCiEi7gE+DmhKakPu/t/mNl9wHx3n3bcuW+hUJAOasueQ/x12XZmLN3Ggk27cYfS3C5MGVrA5KG9OKN3d41ByCmJi1CIBYWCtHeVew/z8vLtTF+6nTnrq2l0KO6ZwZShvZg8tBdnapBaWkGhINIOVO8/wqsrdjB92XbeK6+ivtEpyEzn3NNymTQoj4kDsumWrrvHSfMUCiLtTM3BOl5buYNXVmxnZnk1+4/Uk5JkjCrpwcTSHCYMyGZEYRZpKUlBlypxSKEg0o7V1jeyYONu3lpTyczyKpZv3Ys7ZKQlc1bfnkwozWbigBwGF3QnWdttCPGxeE1EYiQtJYnxpdmML80GYM/BWmavq+a9taHHj2asAiCzcyrj+2czYUA2E0pzKM3tovEIOSmFgkg7kJWRxuShBUweWgDAjr2HmbW2mpnlVby3tpq/Lt8OQF63TkwoDQXE+NJsinpq4Zz8I3UfibRz7s7mXYeYubaKmeVVzF5XTdX+WgAKe3RmQrjFMb5/Dr0y0wOuVmJFYwoi8pHcnQ8q9zNrbTXvra1i9rpd1ByqA6B/bpdQd1NpDuP69yS7a6eAq5VoUSiISEQaG50V2/Y2jUnMXb+L/UfqgdAK63MG5TJF6yMSnkJBRFqlvqGRpVtqmLWumvfKq5mzvpq6Bqd3ZjqTw1twjCruoZsIJRiFgohERc2hOl5fuYPpS7fz9gc7qa1vJK9bJ6YM7cWUYQWc1benpr0mAIWCiETdvsN1vLGqkhlLt/Pm6kqO1DeS0zWNi8/oxSXDChjbrycpyVo8F48UCiISUweO1PPW6p1MX7aNN1ZWcqiugR4ZqVx8RqgFMaE0m1QFRNxQKIhImzlU28Df1uxkxrJtvL6ykv1H6snsnMoFg/O5ZFgvPjYwh04pyUGX2aEpFEQkEIfrGnj3gyqmL9vGqyt2sO9wPd06pXDe6XmcPTCHiQNy6J3VOegyOxxtcyEigUhPTeaCIflcMCSf2vpGZq6tYsbSbby2spJpi7cC0Dc7gwkDckIL5/pnaz1EHFFLQUTaRGOjs3rHvtD+TOVVzDluPcTEcEiM6ddT24HHgLqPRCSu1Tc0smRLTdMeTfM37qa2vpHkJGN4YWZoO/DSbEaV9CA9VeMRp0qhICIJ5XBdAws37ua9tdXMXFvFkooaGhqdtJQkykp6hDbyG5DD8D6ZmvbaCgoFEUlo+w7XMXf9rqbtwFdu2wtA104pjO3Xk/Hhe0aclt9Nq6sjoIFmEUlo3dJTOX9wPucPzgdCtyadvW4XM9dWMWttNa+vqgQgu0sa40qzQzcWKs2hJDtDezSdArUURCQhbdlzKLTTa3kVM9dWsWPvEQB6Z6Y3zWyaUKrtwI9S95GIdBjuzrqqA00zm2atq2bPwb9vB360FTGufzY9uqQFXG0wFAoi0mEd3Q786D0j5qzfxcHaBsxgSEH3pkHrMX170qVTx+hFVyiIiITVNTSypGIPM8tDIbFw4x5qGxpJSTJGFGUxvn/o7nOjinvQOa19Tn9VKIiInMCh2gYWbNzNzLWhe1gv2xKe/pqcxJlFWYwrzWZc/56MKm4/ayQUCiIiEdp3uI75G3Yze101s9aFQqLRIS0liZFFWYwvzWZc/2xGFmcl7MZ+CgURkVbae7iOeet3MWttNbPXV7N8617coVNKEqNLejAu3N00ojCLtJTEWEinUBARiZKag3XM3RAOiXXVrNweCon01CTKSkIL6c4emMPQ3plxu5BOoSAiEiN7DtYyZ/3fQ2LV9n0AZGWkMnFADlOG9uL80/PjatBaK5pFRGIkKyN0C9KLz+gFwM59R5hZXsU7H1TxtzU7eWnJNjLSkrlwSD6XDe/N2YMS5yZDaimIiERRQ6MzZ301Lyzexoxl29hzsI7u6SlMHtqLy0f0YVz/YO5jre4jEZGA1TU08m55FS+8v5VXVuxg/5F68rt34qpRhVxTVkTfnC5tVotCQUQkjhyua+CNVZU8vaCCt1ZX0ugwcUA2nx7XlwsG58W89aBQEBGJUzv2Huap+ZuZOmcTW2sO0zsznevHFvPPZxWT2y02tyZVKIiIxLn6hkZeX1XJY7M28m55FanJxiXDCrhpfAmjintEdQtwzT4SEYlzKclJTbOY1u7cz//N3sjTCyp4/v2tDCnozk3jS/jkyD5tutWGWgoiInHkYG09zy3ayh9nbWDV9n0Myu/KHz47hj5ZnU/pcyNtKSTG+mwRkQ4iIy2F68cWM+NrZ/PwzWVsqznMZx6ey97DdW3y/TENBTObbGarzazczL79ET//upmtMLMlZva6mZXEsh4RkURhZnz89Hwe+HQZG6oOcMfURdQ3NMb8e2MWCmaWDPwGmAIMAa4zsyHHnbYIKHP34cDTwH/Fqh4RkUQ0vjSb+64YyttrdjJ92faYf18sWwpjgHJ3X+futcCfgSuOPcHd33T3g+HD2UBhDOsREUlIFwzJA2Dvodh3IcUyFPoAm485rgi/diK3ADNiWI+IiDQjLqakmtmNQBkw6QQ/vxW4FaC4uLgNKxMR6Vhi2VLYAhQdc1wYfu0fmNkFwHeBy939yEd9kLs/4O5l7l6Wm5sbk2JFRCS2oTAPGGhm/cwsDbgWmHbsCWY2ErifUCBUxrAWERGJQMxCwd3rgTuAl4GVwJPuvtzM7jOzy8On/TfQFXjKzN43s2kn+DgREWkDMR1TcPfpwPTjXvv3Y55fEMvvFxGRltGKZhERaaJQEBGRJgoFERFpolAQEZEmCgUREWmiUBARiXOdkpO5ZFgvintmxPy74mKbCxERObHMjFR+e8PoNvkutRRERKSJQkFERJooFEREpIlCQUREmigURESkiUJBRESaKBRERKSJQkFERJqYuwddQ4uY2U5gYyvfngNURbGcRKBr7hh0zR3DqVxzibs3ez/jhAuFU2Fm8929LOg62pKuuWPQNXcMbXHN6j4SEZEmCgUREWnS0ULhgaALCICuuWPQNXcMMb/mDjWmICIiJ9fRWgoiInIS7TIUzGyyma02s3Iz+/ZH/LyTmT0R/vkcM+vb9lVGVwTX/HUzW2FmS8zsdTMrCaLOaGrumo857yozczNL+JkqkVyzmV0T/rNebmZT27rGaIvg73axmb1pZovCf78vCaLOaDGzh82s0syWneDnZma/DP9+LDGzUVEtwN3b1QNIBtYC/YE0YDEw5LhzvgT8Lvz8WuCJoOtug2s+D8gIP7+9I1xz+LxuwNvAbKAs6Lrb4M95ILAI6BE+zgu67ja45geA28PPhwAbgq77FK/5HGAUsOwEP78EmAEYMA6YE83vb48thTFAubuvc/da4M/AFcedcwXwaPj508D5ZmZtWGO0NXvN7v6mux8MH84GCtu4xmiL5M8Z4PvAj4HDbVlcjERyzV8AfuPuuwHcvbKNa4y2SK7Zge6+XbZ3AAAD2klEQVTh55nA1jasL+rc/W1g10lOuQL4o4fMBrLMrCBa398eQ6EPsPmY44rwax95jrvXAzVAdptUFxuRXPOxbiH0L41E1uw1h5vVRe7+UlsWFkOR/DkPAgaZ2Uwzm21mk9usutiI5JrvBW40swpgOvCVtiktMC39771FdI/mDsbMbgTKgElB1xJLZpYE/Ay4OeBS2loKoS6kcwm1Bt82s2HuvifQqmLrOuARd/+pmY0HHjOzoe7eGHRhiag9thS2AEXHHBeGX/vIc8wshVCTs7pNqouNSK4ZM7sA+C5wubsfaaPaYqW5a+4GDAXeMrMNhPpepyX4YHMkf84VwDR3r3P39cAaQiGRqCK55luAJwHcfRaQTmiPoPYqov/eW6s9hsI8YKCZ9TOzNEIDydOOO2ca8Jnw838C3vDwCE6CavaazWwkcD+hQEj0fmZo5prdvcbdc9y9r7v3JTSOcrm7zw+m3KiI5O/2c4RaCZhZDqHupHVtWWSURXLNm4DzAcxsMKFQ2NmmVbatacBN4VlI44Aad98WrQ9vd91H7l5vZncALxOaufCwuy83s/uA+e4+DXiIUBOznNCAzrXBVXzqIrzm/wa6Ak+Fx9Q3ufvlgRV9iiK85nYlwmt+GbjIzFYADcBd7p6wreAIr/kbwO/N7F8IDTrfnMj/yDOzPxEK9pzwOMk9QCqAu/+O0LjJJUA5cBD4bFS/P4F/70REJMraY/eRiIi0kkJBRESaKBRERKSJQkFERJooFEREpIlCQeQ4ZtZgZu+b2TIze8HMsqL8+Teb2a/Dz+81s3+N5ueLnAqFgsiHHXL3M919KKF1LF8OuiCRtqJQEDm5WRyz2ZiZ3WVm88L72H/vmNdvCr+22MweC792Wfh+HYvM7DUzyw+gfpEWaXcrmkWixcySCW2f8FD4+CJC+wiNIbSX/TQzO4fQvll3AxPcvcrMeoY/4l1gnLu7mX0e+Cah1bcicUuhIPJhnc3sfUIthJXAq+HXLwo/FoWPuxIKiRHAU+5eBeDuR/fCLwSeCO91nwasb5vyRVpP3UciH3bI3c8ESgi1CI6OKRjwo/B4w5nuPsDdHzrJ5/wK+LW7DwNuI7RRm0hcUyiInED4TnVfBb4R3mL9ZeBzZtYVwMz6mFke8AZwtZllh18/2n2Uyd+3NP4MIglA3UciJ+Hui8xsCXCduz8W3pp5Vnin2f3AjeFdO/8D+JuZNRDqXrqZ0B3BnjKz3YSCo18Q1yDSEtolVUREmqj7SEREmigURESkiUJBRESaKBRERKSJQkFERJooFEREpIlCQUREmigURESkyf8HEScr7XSRWNsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert DataFrame to Pandas DataFrame.\n",
    "pr = training_summary.pr.toPandas()\n",
    "\n",
    "# Plot model recall and precision.\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "\n",
    "# Define the labels and show the graph. \n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+-------------+\n",
      "|predictedLabel|label|     features|\n",
      "+--------------+-----+-------------+\n",
      "|           0.0|  0.0|(5,[0],[1.0])|\n",
      "|           0.0|  0.0|(5,[0],[1.0])|\n",
      "|           0.0|  0.0|(5,[0],[1.0])|\n",
      "|           0.0|  0.0|(5,[0],[1.0])|\n",
      "|           0.0|  0.0|(5,[0],[1.0])|\n",
      "+--------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.155801\n",
      "RandomForestClassificationModel (uid=rfc_6a40318e57db) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|prediction|indexedLabel|     features|\n",
      "+----------+------------+-------------+\n",
      "|       0.0|         0.0|    (5,[],[])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "+----------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.168623\n",
      "GBTClassificationModel (uid=GBTClassifier_48f89a5768b1d258dc1c) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Class\" VS \"isWide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isWide2 = df_pre.select('Class', 'Order', 'Family', 'isWide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "# First create a string indexer which converts every string into a number, such as male = 0 and female = 1.\n",
    "# A number will be assigned to every category in the column.\n",
    "Class_indexer = StringIndexer(inputCol='Class',outputCol='ClassIndex')\n",
    "\n",
    "isWide_indexer = StringIndexer(inputCol='isWide',outputCol='label')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# Multiple columns are collapsed into one. \n",
    "# This makes it easier to process when you have multiple classes.\n",
    "Class_encoder = OneHotEncoder(inputCol='ClassIndex',outputCol='ClassVec')\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ClassVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline = Pipeline(stages=[Class_indexer, isWide_indexer, Class_encoder, assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline.fit(df_isWide2)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "#pipe_df_Level = pipeline_model.transform(df_Level)\n",
    "pipe_df = pipeline_model.transform(df_isWide2)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|prediction|indexedLabel|     features|\n",
      "+----------+------------+-------------+\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "|       0.0|         0.0|(5,[0],[1.0])|\n",
      "+----------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.156284\n",
      "GBTClassificationModel (uid=GBTClassifier_4754b9cef4a4ca088244) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(pipe_df)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(pipe_df)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = pipe_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 965.0 failed 1 times, most recent failure: Lost task 0.0 in stage 965.0 (TID 2536, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1949, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-61-5cc80d667d44>\", line 5, in weighted_mean\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py\", line 1869, in sum\n    res = _sum_(a)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1949, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-61-5cc80d667d44>\", line 5, in weighted_mean\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py\", line 1869, in sum\n    res = _sum_(a)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5cc80d667d44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reshape to (key, val) so grouping could work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mweighted_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m ).collect()\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \"\"\"\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 965.0 failed 1 times, most recent failure: Lost task 0.0 in stage 965.0 (TID 2536, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1949, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-61-5cc80d667d44>\", line 5, in weighted_mean\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py\", line 1869, in sum\n    res = _sum_(a)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 174, in main\n    process()\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 169, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/spark-2.1.1-bin-hadoop2.7/python/pyspark/rdd.py\", line 1949, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-61-5cc80d667d44>\", line 5, in weighted_mean\n  File \"/home/ubuntu/.local/lib/python3.5/site-packages/numpy/core/fromnumeric.py\", line 1869, in sum\n    res = _sum_(a)\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from numpy import sum\n",
    "\n",
    "def weighted_mean(df_Level):\n",
    "    vals = list(df_Level)  # save the values from the iterator\n",
    "    sum_of_weights = sum(tup[1] for tup in df_Level)\n",
    "    return sum(1. * tup[0] * tup[1] / sum_of_weights for tup in df_Level)\n",
    "\n",
    "df.rdd.map(\n",
    "    lambda x: (x[0], tuple(x[1:]))  # reshape to (key, val) so grouping could work\n",
    ").groupByKey().mapValues(\n",
    "    weighted_mean\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
